{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BXcEqIi6WJW",
        "outputId": "b44202ef-8af1-4082-f9d5-edf81ee69090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "file_path = \"/content/Updated_Prices_Dataset__Rounded_.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "feature_columns = [\"location\", \"start_hour\", \"end_hour\", \"charging_duration\", \"day_of_week\"]\n",
        "target_column = \"price\"\n",
        "\n",
        "\n",
        "max_price = df[\"price\"].max()\n",
        "df[\"price\"] = df[\"price\"] / max_price\n",
        "\n",
        "\n",
        "label_encoders = {}\n",
        "for col in [\"location\", \"day_of_week\"]:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[[\"start_hour\", \"end_hour\", \"charging_duration\"]] = scaler.fit_transform(df[[\"start_hour\", \"end_hour\", \"charging_duration\"]])\n",
        "\n",
        "\n",
        "X = df[feature_columns].values\n",
        "y = df[target_column].values\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = X.shape[1]  # Number of features\n",
        "output_dim = 1  # Predicting price (single output)\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99  # Discount factor for rewards\n",
        "epsilon = 1.0  # Initial exploration rate\n",
        "epsilon_decay = 0.995\n",
        "epsilon_min = 0.01\n",
        "batch_size = 32\n",
        "memory_size = 10000\n",
        "num_episodes = 500\n",
        "\n",
        "dqn = DQN(input_dim, output_dim)\n",
        "optimizer = optim.Adam(dqn.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.MSELoss()\n",
        "memory = deque(maxlen=memory_size)\n",
        "\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    idx = random.randint(0, len(X) - 1)\n",
        "    state = torch.tensor(X[idx], dtype=torch.float32)\n",
        "    actual_price = y[idx]\n",
        "\n",
        "\n",
        "    if random.random() < epsilon:\n",
        "        action = random.uniform(0, 1)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            action = dqn(state).item()\n",
        "\n",
        "\n",
        "    reward = -abs(actual_price - action)\n",
        "\n",
        "\n",
        "    memory.append((state, action, reward))\n",
        "    loss=0\n",
        "    # Sample batch and train\n",
        "    if len(memory) > batch_size:\n",
        "        batch = random.sample(memory, batch_size)\n",
        "        states, actions, rewards = zip(*batch)\n",
        "\n",
        "        states = torch.stack(states)\n",
        "        actions = torch.tensor(actions, dtype=torch.float32).unsqueeze(1)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        predictions = dqn(states)\n",
        "        loss = loss_fn(predictions, actions)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    if isinstance(loss, torch.Tensor):\n",
        "            print(f\"Episode [{episode}/{num_episodes}], Loss: {loss.item():.4f}\")\n",
        "    else:\n",
        "            print(f\"Episode [{episode}/{num_episodes}], Loss: {loss:.4f}\")  # Print loss as a float if it's not a tensor\n",
        "\n",
        "sample_input = np.array([[3, 14, 16, 1.5, 4]])  # Example: location 3, 2 PM-4 PM, 1.5 hr, Thursday\n",
        "sample_df = pd.DataFrame(sample_input, columns=feature_columns)\n",
        "sample_df[[\"start_hour\", \"end_hour\", \"charging_duration\"]] = scaler.transform(sample_df[[\"start_hour\", \"end_hour\", \"charging_duration\"]])\n",
        "\n",
        "for col in [\"location\", \"day_of_week\"]:\n",
        "    sample_df[col] = sample_df[col].apply(lambda x: label_encoders[col].classes_[0] if x not in label_encoders[col].classes_ else x)\n",
        "    sample_df[col] = label_encoders[col].transform(sample_df[col])\n",
        "\n",
        "sample_tensor = torch.tensor(sample_df.values, dtype=torch.float32)\n",
        "predicted_price = dqn(sample_tensor).item() * max_price  # Convert back\n",
        "\n",
        "print(f\"\\nPredicted Price: ₹{predicted_price:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ialnhpzbjg32",
        "outputId": "c6a6adcb-560c-463c-8a08-dc5080b0fb18"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode [0/500], Loss: 0.0000\n",
            "Episode [1/500], Loss: 0.0000\n",
            "Episode [2/500], Loss: 0.0000\n",
            "Episode [3/500], Loss: 0.0000\n",
            "Episode [4/500], Loss: 0.0000\n",
            "Episode [5/500], Loss: 0.0000\n",
            "Episode [6/500], Loss: 0.0000\n",
            "Episode [7/500], Loss: 0.0000\n",
            "Episode [8/500], Loss: 0.0000\n",
            "Episode [9/500], Loss: 0.0000\n",
            "Episode [10/500], Loss: 0.0000\n",
            "Episode [11/500], Loss: 0.0000\n",
            "Episode [12/500], Loss: 0.0000\n",
            "Episode [13/500], Loss: 0.0000\n",
            "Episode [14/500], Loss: 0.0000\n",
            "Episode [15/500], Loss: 0.0000\n",
            "Episode [16/500], Loss: 0.0000\n",
            "Episode [17/500], Loss: 0.0000\n",
            "Episode [18/500], Loss: 0.0000\n",
            "Episode [19/500], Loss: 0.0000\n",
            "Episode [20/500], Loss: 0.0000\n",
            "Episode [21/500], Loss: 0.0000\n",
            "Episode [22/500], Loss: 0.0000\n",
            "Episode [23/500], Loss: 0.0000\n",
            "Episode [24/500], Loss: 0.0000\n",
            "Episode [25/500], Loss: 0.0000\n",
            "Episode [26/500], Loss: 0.0000\n",
            "Episode [27/500], Loss: 0.0000\n",
            "Episode [28/500], Loss: 0.0000\n",
            "Episode [29/500], Loss: 0.0000\n",
            "Episode [30/500], Loss: 0.0000\n",
            "Episode [31/500], Loss: 0.0000\n",
            "Episode [32/500], Loss: 0.4760\n",
            "Episode [33/500], Loss: 0.3609\n",
            "Episode [34/500], Loss: 0.2584\n",
            "Episode [35/500], Loss: 0.1661\n",
            "Episode [36/500], Loss: 0.1224\n",
            "Episode [37/500], Loss: 0.1203\n",
            "Episode [38/500], Loss: 0.1326\n",
            "Episode [39/500], Loss: 0.1374\n",
            "Episode [40/500], Loss: 0.1286\n",
            "Episode [41/500], Loss: 0.1246\n",
            "Episode [42/500], Loss: 0.1552\n",
            "Episode [43/500], Loss: 0.1513\n",
            "Episode [44/500], Loss: 0.1294\n",
            "Episode [45/500], Loss: 0.1122\n",
            "Episode [46/500], Loss: 0.0973\n",
            "Episode [47/500], Loss: 0.0739\n",
            "Episode [48/500], Loss: 0.0885\n",
            "Episode [49/500], Loss: 0.0899\n",
            "Episode [50/500], Loss: 0.1117\n",
            "Episode [51/500], Loss: 0.0972\n",
            "Episode [52/500], Loss: 0.1119\n",
            "Episode [53/500], Loss: 0.0940\n",
            "Episode [54/500], Loss: 0.0976\n",
            "Episode [55/500], Loss: 0.1141\n",
            "Episode [56/500], Loss: 0.0823\n",
            "Episode [57/500], Loss: 0.0923\n",
            "Episode [58/500], Loss: 0.0788\n",
            "Episode [59/500], Loss: 0.0894\n",
            "Episode [60/500], Loss: 0.0765\n",
            "Episode [61/500], Loss: 0.0970\n",
            "Episode [62/500], Loss: 0.0702\n",
            "Episode [63/500], Loss: 0.0742\n",
            "Episode [64/500], Loss: 0.0725\n",
            "Episode [65/500], Loss: 0.0603\n",
            "Episode [66/500], Loss: 0.0895\n",
            "Episode [67/500], Loss: 0.0667\n",
            "Episode [68/500], Loss: 0.0637\n",
            "Episode [69/500], Loss: 0.0780\n",
            "Episode [70/500], Loss: 0.0594\n",
            "Episode [71/500], Loss: 0.0563\n",
            "Episode [72/500], Loss: 0.0574\n",
            "Episode [73/500], Loss: 0.0842\n",
            "Episode [74/500], Loss: 0.0501\n",
            "Episode [75/500], Loss: 0.0828\n",
            "Episode [76/500], Loss: 0.0940\n",
            "Episode [77/500], Loss: 0.0674\n",
            "Episode [78/500], Loss: 0.0863\n",
            "Episode [79/500], Loss: 0.0762\n",
            "Episode [80/500], Loss: 0.0785\n",
            "Episode [81/500], Loss: 0.0855\n",
            "Episode [82/500], Loss: 0.0892\n",
            "Episode [83/500], Loss: 0.0836\n",
            "Episode [84/500], Loss: 0.0781\n",
            "Episode [85/500], Loss: 0.0737\n",
            "Episode [86/500], Loss: 0.0565\n",
            "Episode [87/500], Loss: 0.0827\n",
            "Episode [88/500], Loss: 0.0643\n",
            "Episode [89/500], Loss: 0.0674\n",
            "Episode [90/500], Loss: 0.0828\n",
            "Episode [91/500], Loss: 0.0488\n",
            "Episode [92/500], Loss: 0.0515\n",
            "Episode [93/500], Loss: 0.0729\n",
            "Episode [94/500], Loss: 0.0420\n",
            "Episode [95/500], Loss: 0.0817\n",
            "Episode [96/500], Loss: 0.0510\n",
            "Episode [97/500], Loss: 0.0908\n",
            "Episode [98/500], Loss: 0.0605\n",
            "Episode [99/500], Loss: 0.0697\n",
            "Episode [100/500], Loss: 0.0781\n",
            "Episode [101/500], Loss: 0.0617\n",
            "Episode [102/500], Loss: 0.0323\n",
            "Episode [103/500], Loss: 0.0687\n",
            "Episode [104/500], Loss: 0.0390\n",
            "Episode [105/500], Loss: 0.0706\n",
            "Episode [106/500], Loss: 0.0520\n",
            "Episode [107/500], Loss: 0.0856\n",
            "Episode [108/500], Loss: 0.1085\n",
            "Episode [109/500], Loss: 0.0481\n",
            "Episode [110/500], Loss: 0.0636\n",
            "Episode [111/500], Loss: 0.0699\n",
            "Episode [112/500], Loss: 0.0679\n",
            "Episode [113/500], Loss: 0.0733\n",
            "Episode [114/500], Loss: 0.0484\n",
            "Episode [115/500], Loss: 0.0714\n",
            "Episode [116/500], Loss: 0.0539\n",
            "Episode [117/500], Loss: 0.0655\n",
            "Episode [118/500], Loss: 0.0564\n",
            "Episode [119/500], Loss: 0.0633\n",
            "Episode [120/500], Loss: 0.0495\n",
            "Episode [121/500], Loss: 0.0457\n",
            "Episode [122/500], Loss: 0.0874\n",
            "Episode [123/500], Loss: 0.0527\n",
            "Episode [124/500], Loss: 0.0627\n",
            "Episode [125/500], Loss: 0.0584\n",
            "Episode [126/500], Loss: 0.0419\n",
            "Episode [127/500], Loss: 0.0610\n",
            "Episode [128/500], Loss: 0.0639\n",
            "Episode [129/500], Loss: 0.0418\n",
            "Episode [130/500], Loss: 0.0701\n",
            "Episode [131/500], Loss: 0.0408\n",
            "Episode [132/500], Loss: 0.0469\n",
            "Episode [133/500], Loss: 0.0334\n",
            "Episode [134/500], Loss: 0.0846\n",
            "Episode [135/500], Loss: 0.0304\n",
            "Episode [136/500], Loss: 0.0598\n",
            "Episode [137/500], Loss: 0.0445\n",
            "Episode [138/500], Loss: 0.0565\n",
            "Episode [139/500], Loss: 0.0303\n",
            "Episode [140/500], Loss: 0.0606\n",
            "Episode [141/500], Loss: 0.0602\n",
            "Episode [142/500], Loss: 0.0638\n",
            "Episode [143/500], Loss: 0.0619\n",
            "Episode [144/500], Loss: 0.0656\n",
            "Episode [145/500], Loss: 0.0673\n",
            "Episode [146/500], Loss: 0.0644\n",
            "Episode [147/500], Loss: 0.0308\n",
            "Episode [148/500], Loss: 0.0308\n",
            "Episode [149/500], Loss: 0.0424\n",
            "Episode [150/500], Loss: 0.0439\n",
            "Episode [151/500], Loss: 0.0446\n",
            "Episode [152/500], Loss: 0.0433\n",
            "Episode [153/500], Loss: 0.0367\n",
            "Episode [154/500], Loss: 0.0694\n",
            "Episode [155/500], Loss: 0.0418\n",
            "Episode [156/500], Loss: 0.0608\n",
            "Episode [157/500], Loss: 0.0381\n",
            "Episode [158/500], Loss: 0.0740\n",
            "Episode [159/500], Loss: 0.0235\n",
            "Episode [160/500], Loss: 0.0271\n",
            "Episode [161/500], Loss: 0.0471\n",
            "Episode [162/500], Loss: 0.0703\n",
            "Episode [163/500], Loss: 0.0540\n",
            "Episode [164/500], Loss: 0.0626\n",
            "Episode [165/500], Loss: 0.0412\n",
            "Episode [166/500], Loss: 0.0827\n",
            "Episode [167/500], Loss: 0.0305\n",
            "Episode [168/500], Loss: 0.0392\n",
            "Episode [169/500], Loss: 0.0415\n",
            "Episode [170/500], Loss: 0.0319\n",
            "Episode [171/500], Loss: 0.0383\n",
            "Episode [172/500], Loss: 0.0483\n",
            "Episode [173/500], Loss: 0.0423\n",
            "Episode [174/500], Loss: 0.0794\n",
            "Episode [175/500], Loss: 0.0386\n",
            "Episode [176/500], Loss: 0.0439\n",
            "Episode [177/500], Loss: 0.0354\n",
            "Episode [178/500], Loss: 0.0384\n",
            "Episode [179/500], Loss: 0.0770\n",
            "Episode [180/500], Loss: 0.0609\n",
            "Episode [181/500], Loss: 0.0542\n",
            "Episode [182/500], Loss: 0.0301\n",
            "Episode [183/500], Loss: 0.0281\n",
            "Episode [184/500], Loss: 0.0734\n",
            "Episode [185/500], Loss: 0.0278\n",
            "Episode [186/500], Loss: 0.0276\n",
            "Episode [187/500], Loss: 0.0545\n",
            "Episode [188/500], Loss: 0.0590\n",
            "Episode [189/500], Loss: 0.0618\n",
            "Episode [190/500], Loss: 0.0320\n",
            "Episode [191/500], Loss: 0.0553\n",
            "Episode [192/500], Loss: 0.0503\n",
            "Episode [193/500], Loss: 0.0570\n",
            "Episode [194/500], Loss: 0.0289\n",
            "Episode [195/500], Loss: 0.0416\n",
            "Episode [196/500], Loss: 0.0323\n",
            "Episode [197/500], Loss: 0.0491\n",
            "Episode [198/500], Loss: 0.0329\n",
            "Episode [199/500], Loss: 0.0574\n",
            "Episode [200/500], Loss: 0.0410\n",
            "Episode [201/500], Loss: 0.0356\n",
            "Episode [202/500], Loss: 0.0493\n",
            "Episode [203/500], Loss: 0.0246\n",
            "Episode [204/500], Loss: 0.0524\n",
            "Episode [205/500], Loss: 0.0403\n",
            "Episode [206/500], Loss: 0.0274\n",
            "Episode [207/500], Loss: 0.0585\n",
            "Episode [208/500], Loss: 0.0518\n",
            "Episode [209/500], Loss: 0.0359\n",
            "Episode [210/500], Loss: 0.0651\n",
            "Episode [211/500], Loss: 0.0438\n",
            "Episode [212/500], Loss: 0.0293\n",
            "Episode [213/500], Loss: 0.0541\n",
            "Episode [214/500], Loss: 0.0367\n",
            "Episode [215/500], Loss: 0.0559\n",
            "Episode [216/500], Loss: 0.0847\n",
            "Episode [217/500], Loss: 0.0449\n",
            "Episode [218/500], Loss: 0.0391\n",
            "Episode [219/500], Loss: 0.0612\n",
            "Episode [220/500], Loss: 0.0419\n",
            "Episode [221/500], Loss: 0.0468\n",
            "Episode [222/500], Loss: 0.0285\n",
            "Episode [223/500], Loss: 0.0341\n",
            "Episode [224/500], Loss: 0.0316\n",
            "Episode [225/500], Loss: 0.0498\n",
            "Episode [226/500], Loss: 0.0387\n",
            "Episode [227/500], Loss: 0.0467\n",
            "Episode [228/500], Loss: 0.0485\n",
            "Episode [229/500], Loss: 0.0280\n",
            "Episode [230/500], Loss: 0.0397\n",
            "Episode [231/500], Loss: 0.0309\n",
            "Episode [232/500], Loss: 0.0415\n",
            "Episode [233/500], Loss: 0.0424\n",
            "Episode [234/500], Loss: 0.0503\n",
            "Episode [235/500], Loss: 0.0290\n",
            "Episode [236/500], Loss: 0.0338\n",
            "Episode [237/500], Loss: 0.0433\n",
            "Episode [238/500], Loss: 0.0420\n",
            "Episode [239/500], Loss: 0.0595\n",
            "Episode [240/500], Loss: 0.0650\n",
            "Episode [241/500], Loss: 0.0472\n",
            "Episode [242/500], Loss: 0.0373\n",
            "Episode [243/500], Loss: 0.0381\n",
            "Episode [244/500], Loss: 0.0224\n",
            "Episode [245/500], Loss: 0.0393\n",
            "Episode [246/500], Loss: 0.0346\n",
            "Episode [247/500], Loss: 0.0585\n",
            "Episode [248/500], Loss: 0.0481\n",
            "Episode [249/500], Loss: 0.0288\n",
            "Episode [250/500], Loss: 0.0449\n",
            "Episode [251/500], Loss: 0.0169\n",
            "Episode [252/500], Loss: 0.0536\n",
            "Episode [253/500], Loss: 0.0363\n",
            "Episode [254/500], Loss: 0.0255\n",
            "Episode [255/500], Loss: 0.0371\n",
            "Episode [256/500], Loss: 0.0377\n",
            "Episode [257/500], Loss: 0.0394\n",
            "Episode [258/500], Loss: 0.0354\n",
            "Episode [259/500], Loss: 0.0453\n",
            "Episode [260/500], Loss: 0.0489\n",
            "Episode [261/500], Loss: 0.0491\n",
            "Episode [262/500], Loss: 0.0410\n",
            "Episode [263/500], Loss: 0.0315\n",
            "Episode [264/500], Loss: 0.0294\n",
            "Episode [265/500], Loss: 0.0424\n",
            "Episode [266/500], Loss: 0.0609\n",
            "Episode [267/500], Loss: 0.0447\n",
            "Episode [268/500], Loss: 0.0372\n",
            "Episode [269/500], Loss: 0.0372\n",
            "Episode [270/500], Loss: 0.0381\n",
            "Episode [271/500], Loss: 0.0564\n",
            "Episode [272/500], Loss: 0.0580\n",
            "Episode [273/500], Loss: 0.0367\n",
            "Episode [274/500], Loss: 0.0423\n",
            "Episode [275/500], Loss: 0.0437\n",
            "Episode [276/500], Loss: 0.0332\n",
            "Episode [277/500], Loss: 0.0514\n",
            "Episode [278/500], Loss: 0.0254\n",
            "Episode [279/500], Loss: 0.0483\n",
            "Episode [280/500], Loss: 0.0532\n",
            "Episode [281/500], Loss: 0.0558\n",
            "Episode [282/500], Loss: 0.0235\n",
            "Episode [283/500], Loss: 0.0327\n",
            "Episode [284/500], Loss: 0.0244\n",
            "Episode [285/500], Loss: 0.0277\n",
            "Episode [286/500], Loss: 0.0406\n",
            "Episode [287/500], Loss: 0.0483\n",
            "Episode [288/500], Loss: 0.0356\n",
            "Episode [289/500], Loss: 0.0196\n",
            "Episode [290/500], Loss: 0.0463\n",
            "Episode [291/500], Loss: 0.0280\n",
            "Episode [292/500], Loss: 0.0330\n",
            "Episode [293/500], Loss: 0.0497\n",
            "Episode [294/500], Loss: 0.0428\n",
            "Episode [295/500], Loss: 0.0863\n",
            "Episode [296/500], Loss: 0.0344\n",
            "Episode [297/500], Loss: 0.0538\n",
            "Episode [298/500], Loss: 0.0298\n",
            "Episode [299/500], Loss: 0.0421\n",
            "Episode [300/500], Loss: 0.0360\n",
            "Episode [301/500], Loss: 0.0260\n",
            "Episode [302/500], Loss: 0.0561\n",
            "Episode [303/500], Loss: 0.0551\n",
            "Episode [304/500], Loss: 0.0234\n",
            "Episode [305/500], Loss: 0.0435\n",
            "Episode [306/500], Loss: 0.0485\n",
            "Episode [307/500], Loss: 0.0230\n",
            "Episode [308/500], Loss: 0.0238\n",
            "Episode [309/500], Loss: 0.0313\n",
            "Episode [310/500], Loss: 0.0452\n",
            "Episode [311/500], Loss: 0.0374\n",
            "Episode [312/500], Loss: 0.0399\n",
            "Episode [313/500], Loss: 0.0377\n",
            "Episode [314/500], Loss: 0.0371\n",
            "Episode [315/500], Loss: 0.0353\n",
            "Episode [316/500], Loss: 0.0249\n",
            "Episode [317/500], Loss: 0.0402\n",
            "Episode [318/500], Loss: 0.0796\n",
            "Episode [319/500], Loss: 0.0609\n",
            "Episode [320/500], Loss: 0.0408\n",
            "Episode [321/500], Loss: 0.0457\n",
            "Episode [322/500], Loss: 0.0403\n",
            "Episode [323/500], Loss: 0.0238\n",
            "Episode [324/500], Loss: 0.0484\n",
            "Episode [325/500], Loss: 0.0480\n",
            "Episode [326/500], Loss: 0.0247\n",
            "Episode [327/500], Loss: 0.0327\n",
            "Episode [328/500], Loss: 0.0335\n",
            "Episode [329/500], Loss: 0.0339\n",
            "Episode [330/500], Loss: 0.0430\n",
            "Episode [331/500], Loss: 0.0399\n",
            "Episode [332/500], Loss: 0.0558\n",
            "Episode [333/500], Loss: 0.0356\n",
            "Episode [334/500], Loss: 0.0403\n",
            "Episode [335/500], Loss: 0.0571\n",
            "Episode [336/500], Loss: 0.0146\n",
            "Episode [337/500], Loss: 0.0269\n",
            "Episode [338/500], Loss: 0.0343\n",
            "Episode [339/500], Loss: 0.0433\n",
            "Episode [340/500], Loss: 0.0424\n",
            "Episode [341/500], Loss: 0.0294\n",
            "Episode [342/500], Loss: 0.0288\n",
            "Episode [343/500], Loss: 0.0567\n",
            "Episode [344/500], Loss: 0.0463\n",
            "Episode [345/500], Loss: 0.0328\n",
            "Episode [346/500], Loss: 0.0381\n",
            "Episode [347/500], Loss: 0.0705\n",
            "Episode [348/500], Loss: 0.0331\n",
            "Episode [349/500], Loss: 0.0361\n",
            "Episode [350/500], Loss: 0.0380\n",
            "Episode [351/500], Loss: 0.0256\n",
            "Episode [352/500], Loss: 0.0195\n",
            "Episode [353/500], Loss: 0.0234\n",
            "Episode [354/500], Loss: 0.0544\n",
            "Episode [355/500], Loss: 0.0359\n",
            "Episode [356/500], Loss: 0.0336\n",
            "Episode [357/500], Loss: 0.0526\n",
            "Episode [358/500], Loss: 0.0309\n",
            "Episode [359/500], Loss: 0.0423\n",
            "Episode [360/500], Loss: 0.0459\n",
            "Episode [361/500], Loss: 0.0264\n",
            "Episode [362/500], Loss: 0.0458\n",
            "Episode [363/500], Loss: 0.0327\n",
            "Episode [364/500], Loss: 0.0323\n",
            "Episode [365/500], Loss: 0.0229\n",
            "Episode [366/500], Loss: 0.0278\n",
            "Episode [367/500], Loss: 0.0423\n",
            "Episode [368/500], Loss: 0.0456\n",
            "Episode [369/500], Loss: 0.0460\n",
            "Episode [370/500], Loss: 0.0285\n",
            "Episode [371/500], Loss: 0.0491\n",
            "Episode [372/500], Loss: 0.0470\n",
            "Episode [373/500], Loss: 0.0214\n",
            "Episode [374/500], Loss: 0.0334\n",
            "Episode [375/500], Loss: 0.0497\n",
            "Episode [376/500], Loss: 0.0393\n",
            "Episode [377/500], Loss: 0.0273\n",
            "Episode [378/500], Loss: 0.0500\n",
            "Episode [379/500], Loss: 0.0296\n",
            "Episode [380/500], Loss: 0.0336\n",
            "Episode [381/500], Loss: 0.0245\n",
            "Episode [382/500], Loss: 0.0509\n",
            "Episode [383/500], Loss: 0.0258\n",
            "Episode [384/500], Loss: 0.0515\n",
            "Episode [385/500], Loss: 0.0634\n",
            "Episode [386/500], Loss: 0.0231\n",
            "Episode [387/500], Loss: 0.0404\n",
            "Episode [388/500], Loss: 0.0438\n",
            "Episode [389/500], Loss: 0.0224\n",
            "Episode [390/500], Loss: 0.0430\n",
            "Episode [391/500], Loss: 0.0269\n",
            "Episode [392/500], Loss: 0.0140\n",
            "Episode [393/500], Loss: 0.0260\n",
            "Episode [394/500], Loss: 0.0300\n",
            "Episode [395/500], Loss: 0.0349\n",
            "Episode [396/500], Loss: 0.0276\n",
            "Episode [397/500], Loss: 0.0111\n",
            "Episode [398/500], Loss: 0.0343\n",
            "Episode [399/500], Loss: 0.0409\n",
            "Episode [400/500], Loss: 0.0292\n",
            "Episode [401/500], Loss: 0.0352\n",
            "Episode [402/500], Loss: 0.0253\n",
            "Episode [403/500], Loss: 0.0221\n",
            "Episode [404/500], Loss: 0.0320\n",
            "Episode [405/500], Loss: 0.0365\n",
            "Episode [406/500], Loss: 0.0320\n",
            "Episode [407/500], Loss: 0.0218\n",
            "Episode [408/500], Loss: 0.0139\n",
            "Episode [409/500], Loss: 0.0402\n",
            "Episode [410/500], Loss: 0.0505\n",
            "Episode [411/500], Loss: 0.0332\n",
            "Episode [412/500], Loss: 0.0227\n",
            "Episode [413/500], Loss: 0.0141\n",
            "Episode [414/500], Loss: 0.0195\n",
            "Episode [415/500], Loss: 0.0233\n",
            "Episode [416/500], Loss: 0.0524\n",
            "Episode [417/500], Loss: 0.0242\n",
            "Episode [418/500], Loss: 0.0251\n",
            "Episode [419/500], Loss: 0.0435\n",
            "Episode [420/500], Loss: 0.0350\n",
            "Episode [421/500], Loss: 0.0186\n",
            "Episode [422/500], Loss: 0.0385\n",
            "Episode [423/500], Loss: 0.0377\n",
            "Episode [424/500], Loss: 0.0185\n",
            "Episode [425/500], Loss: 0.0289\n",
            "Episode [426/500], Loss: 0.0230\n",
            "Episode [427/500], Loss: 0.0257\n",
            "Episode [428/500], Loss: 0.0212\n",
            "Episode [429/500], Loss: 0.0413\n",
            "Episode [430/500], Loss: 0.0212\n",
            "Episode [431/500], Loss: 0.0262\n",
            "Episode [432/500], Loss: 0.0203\n",
            "Episode [433/500], Loss: 0.0224\n",
            "Episode [434/500], Loss: 0.0344\n",
            "Episode [435/500], Loss: 0.0310\n",
            "Episode [436/500], Loss: 0.0241\n",
            "Episode [437/500], Loss: 0.0271\n",
            "Episode [438/500], Loss: 0.0337\n",
            "Episode [439/500], Loss: 0.0430\n",
            "Episode [440/500], Loss: 0.0326\n",
            "Episode [441/500], Loss: 0.0156\n",
            "Episode [442/500], Loss: 0.0216\n",
            "Episode [443/500], Loss: 0.0339\n",
            "Episode [444/500], Loss: 0.0239\n",
            "Episode [445/500], Loss: 0.0456\n",
            "Episode [446/500], Loss: 0.0398\n",
            "Episode [447/500], Loss: 0.0271\n",
            "Episode [448/500], Loss: 0.0418\n",
            "Episode [449/500], Loss: 0.0601\n",
            "Episode [450/500], Loss: 0.0215\n",
            "Episode [451/500], Loss: 0.0217\n",
            "Episode [452/500], Loss: 0.0338\n",
            "Episode [453/500], Loss: 0.0214\n",
            "Episode [454/500], Loss: 0.0261\n",
            "Episode [455/500], Loss: 0.0348\n",
            "Episode [456/500], Loss: 0.0355\n",
            "Episode [457/500], Loss: 0.0334\n",
            "Episode [458/500], Loss: 0.0363\n",
            "Episode [459/500], Loss: 0.0320\n",
            "Episode [460/500], Loss: 0.0227\n",
            "Episode [461/500], Loss: 0.0407\n",
            "Episode [462/500], Loss: 0.0302\n",
            "Episode [463/500], Loss: 0.0217\n",
            "Episode [464/500], Loss: 0.0447\n",
            "Episode [465/500], Loss: 0.0474\n",
            "Episode [466/500], Loss: 0.0203\n",
            "Episode [467/500], Loss: 0.0381\n",
            "Episode [468/500], Loss: 0.0376\n",
            "Episode [469/500], Loss: 0.0322\n",
            "Episode [470/500], Loss: 0.0402\n",
            "Episode [471/500], Loss: 0.0594\n",
            "Episode [472/500], Loss: 0.0148\n",
            "Episode [473/500], Loss: 0.0265\n",
            "Episode [474/500], Loss: 0.0232\n",
            "Episode [475/500], Loss: 0.0178\n",
            "Episode [476/500], Loss: 0.0260\n",
            "Episode [477/500], Loss: 0.0421\n",
            "Episode [478/500], Loss: 0.0306\n",
            "Episode [479/500], Loss: 0.0168\n",
            "Episode [480/500], Loss: 0.0303\n",
            "Episode [481/500], Loss: 0.0432\n",
            "Episode [482/500], Loss: 0.0393\n",
            "Episode [483/500], Loss: 0.0448\n",
            "Episode [484/500], Loss: 0.0191\n",
            "Episode [485/500], Loss: 0.0353\n",
            "Episode [486/500], Loss: 0.0297\n",
            "Episode [487/500], Loss: 0.0298\n",
            "Episode [488/500], Loss: 0.0246\n",
            "Episode [489/500], Loss: 0.0542\n",
            "Episode [490/500], Loss: 0.0506\n",
            "Episode [491/500], Loss: 0.0375\n",
            "Episode [492/500], Loss: 0.0210\n",
            "Episode [493/500], Loss: 0.0215\n",
            "Episode [494/500], Loss: 0.0633\n",
            "Episode [495/500], Loss: 0.0496\n",
            "Episode [496/500], Loss: 0.0166\n",
            "Episode [497/500], Loss: 0.0438\n",
            "Episode [498/500], Loss: 0.0144\n",
            "Episode [499/500], Loss: 0.0269\n",
            "\n",
            "Predicted Price: ₹98.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = np.array([[3, 15, 16, 2, 4]])  # Example: location 3, 2 PM-4 PM, 1.5 hr, Thursday\n",
        "sample_df = pd.DataFrame(sample_input, columns=feature_columns)\n",
        "sample_df[[\"start_hour\", \"end_hour\", \"charging_duration\"]] = scaler.transform(sample_df[[\"start_hour\", \"end_hour\", \"charging_duration\"]])\n",
        "\n",
        "# ✅ Handle unseen labels safely\n",
        "for col in [\"location\", \"day_of_week\"]:\n",
        "    sample_df[col] = sample_df[col].apply(lambda x: label_encoders[col].classes_[0] if x not in label_encoders[col].classes_ else x)\n",
        "    sample_df[col] = label_encoders[col].transform(sample_df[col])\n",
        "\n",
        "sample_tensor = torch.tensor(sample_df.values, dtype=torch.float32)\n",
        "predicted_price = dqn(sample_tensor).item() * max_price  # Convert back\n",
        "\n",
        "print(f\"\\nPredicted Price: ₹{predicted_price:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWzxIP_e7cyy",
        "outputId": "98db9432-44c2-4dad-d72a-d25978322371"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted Price: ₹11.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save model\n",
        "\n",
        "import numpy as np\n",
        "# Save the trained model\n",
        "torch.save(dqn.state_dict(), 'dqn_model.pth')\n",
        "\n",
        "# Save other necessary components for inference\n",
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
        "np.save('max_price.npy', max_price)\n"
      ],
      "metadata": {
        "id": "QSA79Om38T4V"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}